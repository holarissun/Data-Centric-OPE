
# DataCOPE
Code for Data-Centric Off-Policy Evaluation Without an Environment



#### Usage:

#### 1. Generating synthetic data for OPE with classification or regression tasks
```
data_gen.py
```
```
parser.add_argument('--data_dir', type=str, default='data_dir')
parser.add_argument('--dataset', type=str, default='Breast_cancer')
parser.add_argument('--train_proportion', type=float, default=0.8)
parser.add_argument('--noise', type=float, default=0.0)
```
generates synthetic contextual log bandit dataset based on UCI dataset.
- behavior policies are based on SGDRegressor (for regression) or LogisticRegression (for classification)
- evaluation policies are generated by MLPRegressor or MLPClassifier
- actions are perturbated with noise, controlled by the args --noise
- then (x, a, r^a) are saved to files, as training set and test set, separately.

#### 2. Benchmarking OPE algorithms.

    Implemented OPE methods
    - Direct method
    - Replay method
    - Doubly Robust
    - Self-Normalized Doubly Robust
    - IPW-Clip
    - SN-DR

```
ope_algos_reg.py
benchmark_ope_reg.py
```

#### 3. Case Study on biased dataset: biased $\pi_b$ leads to biased performance for OPE.
```
ot_data_processing.py
```